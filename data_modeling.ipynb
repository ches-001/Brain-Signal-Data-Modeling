{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EEG (Electroencephalography) / NIRS (Near Infrared Spectroscopy) Data Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from data_preparation.dataset import SignalDataset\n",
    "from architecture.net import ClassifierNet\n",
    "from pipeline.train import TrainingPipeline\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.GaussianBlur(sigma=(0.001, 2), kernel_size=5),\n",
    "])\n",
    "\n",
    "transforms_p = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"nback\"\n",
    "t_size = 300\n",
    "train_size = 0.9\n",
    "data_dir = \"data/samples\"\n",
    "signal = \"EEG\"\n",
    "hemoglobin = None\n",
    "onehot_labels = False\n",
    "shuffle = True\n",
    "use_spectrogram = False\n",
    "use_rp = True\n",
    "excluded_classes = [\n",
    "    \"0-back target\",\n",
    "    \"0-back session\",\n",
    "    \"2-back session\", \n",
    "    \"3-back session\",\n",
    "    \"3-back target\",\n",
    "    \"3-back non-target\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv(os.path.join(data_dir, f\"{signal.upper()}.csv\"))\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = SignalDataset(\n",
    "    data_dir, \n",
    "    meta_df=meta_df,\n",
    "    signal=signal,\n",
    "    task=task, \n",
    "    hemoglobin=hemoglobin,\n",
    "    excluded_classes=excluded_classes,\n",
    "    sample_size=train_size, \n",
    "    t_size=t_size,\n",
    "    use_spectrogram=use_spectrogram,\n",
    "    use_rp=use_rp,\n",
    "    onehot_labels=onehot_labels,\n",
    "    shuffle=shuffle,\n",
    "    transforms=transforms,\n",
    "    transforms_p=transforms_p\n",
    ")\n",
    "\n",
    "eval_dataset = SignalDataset(\n",
    "    data_dir,\n",
    "    meta_df=meta_df,\n",
    "    signal=signal, \n",
    "    task=task, \n",
    "    hemoglobin=hemoglobin,\n",
    "    excluded_paths=training_dataset.meta_df[\"path\"].tolist(), \n",
    "    excluded_classes=excluded_classes,\n",
    "    t_size=t_size,\n",
    "    use_spectrogram=use_spectrogram,\n",
    "    use_rp=use_rp,\n",
    "    onehot_labels=onehot_labels,\n",
    "    shuffle=shuffle,\n",
    ")\n",
    "\n",
    "print(f\"Number of train samples: {len(training_dataset)}\")\n",
    "print(f\"Number of eval samples: {len(eval_dataset)}\")\n",
    "print(f\"Number of classes: {len(training_dataset.get_label_names())}\")\n",
    "print(f\"Class names: {training_dataset.get_label_names()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualise sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_signal, label = training_dataset[0]\n",
    "print(sample_signal.shape)\n",
    "print(label.shape)\n",
    "print(f\"class label: {label.item()}\")\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "if training_dataset.use_spectrogram:\n",
    "    if training_dataset.avg_spectrogram_ch:\n",
    "        print(f\"spectrogram kwargs: {training_dataset.spectrogram_kwargs}\")\n",
    "        plt.imshow(sample_signal.squeeze())\n",
    "        plt.title(f\"Averaged Channel {signal} Spctrogram\")\n",
    "        plt.xlabel(\"Timesteps\")\n",
    "        plt.ylabel(\"Channel readings\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "elif training_dataset.use_rp:\n",
    "    plt.title(f\"Recurrence plot\", cmap=\"gray\")\n",
    "    plt.imshow(sample_signal.squeeze())\n",
    "\n",
    "else:\n",
    "    for ch in sample_signal.squeeze():\n",
    "        plt.plot(ch)\n",
    "    plt.title(f\"All channels {signal} Time signals\")\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Channel readings\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Check Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.countplot(training_dataset.meta_df, x=\"class_name\", ax=axs[0])\n",
    "axs[0].set_title(\"class count plot for training samples\")\n",
    "\n",
    "sns.countplot(eval_dataset.meta_df, x=\"class_name\", ax=axs[1])\n",
    "axs[1].set_title(\"class count plot for evaluation samples\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define DataLoader and Account for class Imbalance with a Random Weighted Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 32\n",
    "\n",
    "_, train_sample_weights = training_dataset.get_sample_weights()\n",
    "train_dataloader = DataLoader(\n",
    "    training_dataset, \n",
    "    num_workers=num_workers, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    sampler=WeightedRandomSampler(train_sample_weights, len(training_dataset), replacement=True)\n",
    ")\n",
    "\n",
    "_, eval_sample_weights = eval_dataset.get_sample_weights()\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=WeightedRandomSampler(eval_sample_weights, len(eval_dataset), replacement=True)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define the Relevant Hyper-parameters and Objects for Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "num_classes = len(training_dataset.get_label_names())\n",
    "dropout = 0.3\n",
    "network = \"resnet18\"\n",
    "pretrained_weights = None #\"DEFAULT\"\n",
    "track_grads = True\n",
    "lr = 1e-4\n",
    "min_lr = 1e-6\n",
    "weight_decay = 7e-6\n",
    "betas = (0.9, 0.9999)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "T_0 = 40\n",
    "T_mult = 2\n",
    "model_folder = \"saved_model\"\n",
    "model_name = f\"{signal}_{task}_model.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3407)\n",
    "torch.cuda.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassifierNet(in_channels, num_classes, dropout, network, pretrained_weights, track_grads)\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=lr, weight_decay=weight_decay, betas=betas)\n",
    "lossfunc = nn.BCELoss() if onehot_labels else nn.CrossEntropyLoss()\n",
    "\n",
    "# define pipeline\n",
    "pipeline = TrainingPipeline(\n",
    "    classifier, \n",
    "    lossfunc, \n",
    "    optimizer, \n",
    "    device, \n",
    "    weight_init=False, \n",
    "    dirname=model_folder, \n",
    "    filename=model_name,\n",
    "    onehot_labels=onehot_labels,\n",
    ")\n",
    "\n",
    "# lr scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    pipeline.optimizer, T_0=T_0, T_mult=T_mult, eta_min=min_lr, verbose=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch: {epoch+1} / {epochs}\")\n",
    "    pipeline.train(train_dataloader, verbose=True)\n",
    "    pipeline.evaluate(eval_dataloader, verbose=True)\n",
    "    lr_scheduler.step()\n",
    "    print(\"-\"*130)\n",
    "\n",
    "pipeline.plot_metrics(\"train\")\n",
    "pipeline.plot_metrics(\"eval\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
